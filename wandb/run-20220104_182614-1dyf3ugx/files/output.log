Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Mean reward: -17.4 Num episodes: 4
Logging to ./gol_results/PPO_4
Traceback (most recent call last):
  File "test.py", line 137, in <module>
    sb3_test()
  File "test.py", line 102, in sb3_test
    model.learn(total_timesteps=timesteps, log_interval=4, callback=WandbCallback())
  File "/home/ben/anaconda3/envs/openai/lib/python3.6/site-packages/stable_baselines3/ppo/ppo.py", line 308, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/ben/anaconda3/envs/openai/lib/python3.6/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 237, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/ben/anaconda3/envs/openai/lib/python3.6/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 169, in collect_rollouts
    actions, values, log_probs = self.policy.forward(obs_tensor)
  File "/home/ben/anaconda3/envs/openai/lib/python3.6/site-packages/stable_baselines3/common/policies.py", line 592, in forward
    log_prob = distribution.log_prob(actions)
  File "/home/ben/anaconda3/envs/openai/lib/python3.6/site-packages/stable_baselines3/common/distributions.py", line 383, in log_prob
    return self.distribution.log_prob(actions).sum(dim=1)
  File "/home/ben/anaconda3/envs/openai/lib/python3.6/site-packages/torch/distributions/bernoulli.py", line 95, in log_prob
    return -binary_cross_entropy_with_logits(logits, value, reduction='none')
  File "/home/ben/anaconda3/envs/openai/lib/python3.6/site-packages/torch/nn/functional.py", line 2960, in binary_cross_entropy_with_logits
    return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)
KeyboardInterrupt