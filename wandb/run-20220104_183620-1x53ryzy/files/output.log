Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Mean reward: -26.6 Num episodes: 3
Logging to ./gol_results/PPO_5
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 281         |
|    ep_rew_mean          | -18         |
| time/                   |             |
|    fps                  | 782         |
|    iterations           | 4           |
|    time_elapsed         | 10          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.011412595 |
|    clip_fraction        | 0.0857      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.22       |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.272       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 1.63        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 274         |
|    ep_rew_mean          | -17.3       |
| time/                   |             |
|    fps                  | 756         |
|    iterations           | 8           |
|    time_elapsed         | 21          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.010445285 |
|    clip_fraction        | 0.0927      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.26        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0392     |
|    value_loss           | 3.78        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 266         |
|    ep_rew_mean          | -16.5       |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 12          |
|    time_elapsed         | 32          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.012030745 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.11       |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.952       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0438     |
|    value_loss           | 2.7         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 297         |
|    ep_rew_mean          | -19.6       |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 16          |
|    time_elapsed         | 43          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.015916001 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.84        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.055      |
|    value_loss           | 2.96        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 299         |
|    ep_rew_mean          | -19.8       |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 20          |
|    time_elapsed         | 54          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.017993176 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 0.759       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.335       |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.053      |
|    value_loss           | 1.96        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 343         |
|    ep_rew_mean          | -24.2       |
| time/                   |             |
|    fps                  | 757         |
|    iterations           | 24          |
|    time_elapsed         | 64          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.019517846 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.89       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.251       |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.055      |
|    value_loss           | 2.41        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 361         |
|    ep_rew_mean          | -26         |
| time/                   |             |
|    fps                  | 756         |
|    iterations           | 28          |
|    time_elapsed         | 75          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.018659526 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.79       |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.22        |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0526     |
|    value_loss           | 3.26        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 343         |
|    ep_rew_mean          | -24.2       |
| time/                   |             |
|    fps                  | 756         |
|    iterations           | 32          |
|    time_elapsed         | 86          |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.018571038 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.7        |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.7         |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0603     |
|    value_loss           | 3.77        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 322         |
|    ep_rew_mean          | -22.1       |
| time/                   |             |
|    fps                  | 759         |
|    iterations           | 36          |
|    time_elapsed         | 97          |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.018719273 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.58       |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.691       |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0559     |
|    value_loss           | 2.97        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 276         |
|    ep_rew_mean          | -17.5       |
| time/                   |             |
|    fps                  | 760         |
|    iterations           | 40          |
|    time_elapsed         | 107         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.019509211 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.48       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.18        |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.055      |
|    value_loss           | 4.25        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 264         |
|    ep_rew_mean          | -16.3       |
| time/                   |             |
|    fps                  | 761         |
|    iterations           | 44          |
|    time_elapsed         | 118         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.024788579 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.42       |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.0003      |
|    loss                 | 1.1         |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0589     |
|    value_loss           | 2.6         |
-----------------------------------------
Traceback (most recent call last):
  File "test.py", line 137, in <module>
    sb3_test()
  File "test.py", line 102, in sb3_test
    model.learn(total_timesteps=timesteps, log_interval=4, callback=WandbCallback())
  File "/home/ben/anaconda3/envs/openai/lib/python3.6/site-packages/stable_baselines3/ppo/ppo.py", line 308, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/ben/anaconda3/envs/openai/lib/python3.6/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 257, in learn
    self.train()
  File "/home/ben/anaconda3/envs/openai/lib/python3.6/site-packages/stable_baselines3/ppo/ppo.py", line 189, in train
    for rollout_data in self.rollout_buffer.get(self.batch_size):
  File "/home/ben/anaconda3/envs/openai/lib/python3.6/site-packages/stable_baselines3/common/buffers.py", line 450, in get
    yield self._get_samples(indices[start_idx : start_idx + batch_size])
  File "/home/ben/anaconda3/envs/openai/lib/python3.6/site-packages/stable_baselines3/common/buffers.py", line 462, in _get_samples
    return RolloutBufferSamples(*tuple(map(self.to_torch, data)))
  File "/home/ben/anaconda3/envs/openai/lib/python3.6/site-packages/stable_baselines3/common/buffers.py", line 134, in to_torch
    return th.tensor(array).to(self.device)
KeyboardInterrupt